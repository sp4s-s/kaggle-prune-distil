{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc02889",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install --upgrade pip\n",
    "! pip install datasets trl wandb tensorboard peft -qU\n",
    "! pip install flash-attn --no-build-isolation -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dd8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from accelerate import Accelerator\n",
    "import wandb\n",
    "import math, os, random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908739a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_environment():\n",
    "    # wandb.init(project=\"webinstructsub-finetuning-small\", entity=\"fish\")\n",
    "    os.environ[\"WANDB_DISABLED\"] = 'true'\n",
    "    return Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f5fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name):\n",
    "    model_kwargs = {\n",
    "        \"torch_dtype\": torch.bfloat16\n",
    "    }\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M-Instruct\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd6b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_train_dataset(start_idx, num_rows):\n",
    "    dataset = load_dataset(\"TIGER-Lab/WebInstruct52k\", split=\"train\", streaming=True)\n",
    "    dataset = dataset.skip(start_idx).take(num_rows)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aba718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"question\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"answer\"]}\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377eeb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_eval_dataset():\n",
    "    dataset = load_dataset(\"TIGER-Lab/WebInstruct52k\", split=\"train\") # Assuming split=\"train\" for eval\n",
    "    total_rows = len(dataset)\n",
    "    # Generate a list of random indices\n",
    "    random_indices = random.sample(range(total_rows), eval_rows)\n",
    "    # Select the random rows\n",
    "    dataset = dataset.select(random_indices)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3cfdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction_for_trainer(example):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M-Instruct\") # Re-instantiate tokenizer here if not passed\n",
    "    return tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=2048,\n",
    "        tokenize_file=False # Assuming this is meant to be tokenize=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e91f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_lr_scheduler(optimizer, num_warmup_steps, num_training_steps, initial_phase_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < initial_phase_steps:\n",
    "            return 1.0 # Constant learning rate for initial phase\n",
    "        else:\n",
    "            # Cosine annealing for the remaining steps\n",
    "            return 0.5 * (1.0 + math.cos(math.pi * (current_step - initial_phase_steps) / (num_training_steps - initial_phase_steps)))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba962cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Model configuration\n",
    "    model_name = \"Trellis/SmolLM-135M-Instruct-layer-pruned-90M-raw\"\n",
    "    # Distilled model\n",
    "    # model_name = \"Trellis/90M-base\" # This line is commented out in the image but shows the intent\n",
    "\n",
    "    # Select rows to train on\n",
    "    initial_rows = 5000\n",
    "    annealing_rows = 1000\n",
    "    eval_rows = 1000 # Only 10000 rows for evaluation\n",
    "\n",
    "    batch_size = 8\n",
    "    ga = 4 # Gradient accumulation steps\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    accelerator = setup_environment()\n",
    "\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "    print(model.device)\n",
    "\n",
    "    # Combined training dataset (streaming)\n",
    "    total_rows = initial_rows + annealing_rows\n",
    "    train_dataset = load_and_preprocess_train_dataset(0, total_rows)\n",
    "    train_dataset = train_dataset.map(format_instruction, batched=False)\n",
    "    formatted_dataset = train_dataset.map(format_instruction_for_trainer)\n",
    "\n",
    "    # Evaluation dataset (non-streaming, last 1000 rows)\n",
    "    eval_dataset = load_and_preprocess_eval_dataset()\n",
    "    eval_dataset = eval_dataset.map(format_instruction, batched=False)\n",
    "    eval_dataset = eval_dataset.map(format_instruction_for_trainer)\n",
    "\n",
    "    # Calculate steps\n",
    "    num_epochs = 1 # Example value, from image\n",
    "    total_steps = (total_rows * num_epochs) // (batch_size * ga)\n",
    "    initial_steps = (initial_rows * num_epochs) // (batch_size * ga)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_name = f\"{model_name.split('/')[-1]}-SFT-(total_rows-lr_{learning_rate})-{timestamp}\"\n",
    "    output_dir = f\"./results/{run_name}\"\n",
    "\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        run_name=run_name,\n",
    "        logging_dir=f\"./logs/{run_name}\",\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        report_to=\"tensorboard\",\n",
    "        num_train_epochs=num_epochs, # Set to None when using max_steps\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_steps=20, # Example value, needs to be calculated\n",
    "        logging_steps=int(total_steps * 0.1), # Example value, needs to be calculated\n",
    "        eval_steps=int(total_steps * 0.1), # Example value, needs to be calculated\n",
    "        save_steps=int(total_steps * 0.1), # Example value, needs to be calculated\n",
    "        learning_rate=learning_rate,\n",
    "        bf16=True,\n",
    "        max_steps=total_steps,\n",
    "        gradient_accumulation_steps=ga,\n",
    "    )\n",
    "\n",
    "    # Custom learning rate scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "    lr_scheduler = get_custom_lr_scheduler(optimizer, training_args.warmup_steps, total_steps, initial_steps)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=formatted_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=2048,\n",
    "        formatting_func=format_instruction_for_trainer,\n",
    "        optimizers=(optimizer, lr_scheduler) # Use custom optimizer and scheduler\n",
    "    )\n",
    "\n",
    "    trainer = accelerator.prepare(trainer)\n",
    "\n",
    "    print(f\"Starting instruction fine-tuning on {total_rows} rows of data...\")\n",
    "    trainer.train()\n",
    "    print(\"Instruction fine-tuning completed. Saving model...\")\n",
    "\n",
    "    trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867a677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da5664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c25238b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea79400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad92316a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2377c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b7b7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f3644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a568db9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc6b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3fbf81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb169ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488f8cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b1490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ceec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a5e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3498817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc31590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ff05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d6804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
