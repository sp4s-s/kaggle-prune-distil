{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc875c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, get_scheduler\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, from_hub=False, subfolder=None):\n",
    "    print(f\"Loading model_path from {'Hugging Face Hub' if from_hub else 'local path'}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, subfolder=subfolder)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, subfolder=subfolder, torch_dtype=torch.bfloat16)\n",
    "    print(tokenizer)\n",
    "    return tokenizer, model\n",
    "\n",
    "def setup_device():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "def generate_response(question, tokenizer, model, device):\n",
    "    # \"EASY\" PROMPTING, BETTER FOR MODELS NOT INSTRUCTION TUNED.\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    prompt = question\n",
    "    print(\"Input:\")\n",
    "    print(prompt)\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) # CRITICAL for instruction finetuned models\n",
    "    print(input_text)\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate the model's response\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.3,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    # Decode the output but remove the input portion from the decoded output\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=False) # SET TO FALSE COS IT'S EASIER TO STRIP\n",
    "\n",
    "    # Remove the input text from the model's response - ADDED THIS TO AVOID REPETITION\n",
    "    response = decoded_output[len(input_text):].strip() # Strip any leading/trailing spaces\n",
    "    print(\"\\nModel Response:\")\n",
    "    print(response)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Ask user whether to load from local path or Hugging Face Hub\n",
    "    load_option = input(\"Load model from (1) local path or (2) Hugging Face Hub (manual) or (3) instruct layer-pruned or (4) hidden-pruned or (5) 99-v9 instruct: \")\n",
    "\n",
    "    if load_option == \"B\": # From initial images, this looked like \"B\" was a specific path\n",
    "        model_path = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "        from_hub = True\n",
    "    elif load_option == \"1\":\n",
    "        model_path = \"Trellis/SmolLM-135M-Instruct-layer-pruned-90M-raw-SFT-6000rows-lr0.001-20240924\"\n",
    "        from_hub = False\n",
    "    elif load_option == \"2\":\n",
    "        model_path = \"RS545837/TrellisLM-smolla-distill-1\"\n",
    "        from_hub = True\n",
    "    elif load_option == \"3\":\n",
    "        model_path = \"Trellis/SmolLM-100M-layer-hidden-pruned\"\n",
    "        from_hub = True\n",
    "    elif load_option == \"4\":\n",
    "        model_path = \"Trellis/SmolLM-100M-Instruct-layer-hidden-pruned\"\n",
    "        from_hub = True\n",
    "    elif load_option == \"5\":\n",
    "        model_path = \"Trellis/99-instruct-v9\"\n",
    "        from_hub = True\n",
    "    else:\n",
    "        print(\"Invalid option. Defaulting to local path.\")\n",
    "        model_path = \"./finetuned_model_small_messages\"\n",
    "        from_hub = False\n",
    "\n",
    "    device = setup_device()\n",
    "    tokenizer, model = load_model(model_path, from_hub)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # print(f\"Student model: {model}\") # This line was commented out in one image\n",
    "\n",
    "    questions = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"One, two, three, \",\n",
    "        \"After dinner, I \",\n",
    "        \"The wheels on the bus go \",\n",
    "        \"What is the universe?\",\n",
    "        \"How do you calculate the derivative of a function?\",\n",
    "        \"How to kill?\",\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        generate_response(question, tokenizer, model, device)\n",
    "        print(\"=\"*50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa4046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec396e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb2c9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea68bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b229373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e69ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a615d102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769a14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc22f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dcc3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c497532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8fbff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
